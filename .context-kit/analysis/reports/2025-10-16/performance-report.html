<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Performance Analysis Report - tkr-docusearch</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 2rem;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 1rem;
        }
        .header .icon {
            font-size: 3rem;
        }
        .header p {
            font-size: 1.1rem;
            opacity: 0.9;
        }
        .content {
            padding: 2rem;
        }
        .section {
            margin-bottom: 3rem;
        }
        .section h2 {
            font-size: 1.8rem;
            color: #667eea;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #667eea;
        }
        .section h3 {
            font-size: 1.3rem;
            color: #764ba2;
            margin: 1.5rem 0 1rem;
        }
        .score-card {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }
        .score-item {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 1.5rem;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .score-item .value {
            font-size: 2.5rem;
            font-weight: bold;
            color: #667eea;
            display: block;
        }
        .score-item .label {
            font-size: 0.9rem;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        .score-excellent { background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%) !important; }
        .score-good { background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%) !important; }
        .score-warning { background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%) !important; }
        .score-critical { background: linear-gradient(135deg, #feada6 0%, #f5efef 100%) !important; }
        .issue {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 1.5rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .issue.high { border-left-color: #e74c3c; background: #ffebee; }
        .issue.medium { border-left-color: #f39c12; background: #fff3e0; }
        .issue.low { border-left-color: #3498db; background: #e3f2fd; }
        .issue-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.5rem;
        }
        .issue-title {
            font-weight: bold;
            font-size: 1.1rem;
        }
        .severity {
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.85rem;
            font-weight: bold;
            text-transform: uppercase;
        }
        .severity.critical { background: #e74c3c; color: white; }
        .severity.high { background: #f39c12; color: white; }
        .severity.medium { background: #3498db; color: white; }
        .severity.low { background: #95a5a6; color: white; }
        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 1rem;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.4;
        }
        .code-block .line-num {
            color: #636d83;
            margin-right: 1rem;
            user-select: none;
        }
        .fix-recommendation {
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }
        .fix-recommendation strong {
            color: #2e7d32;
        }
        .stat-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 1.5rem 0;
        }
        .stat-box {
            background: #f8f9fa;
            padding: 1.5rem;
            border-radius: 8px;
            border: 1px solid #e0e0e0;
        }
        .stat-box h4 {
            color: #667eea;
            margin-bottom: 0.5rem;
        }
        .stat-box ul {
            list-style: none;
            padding-left: 0;
        }
        .stat-box li {
            padding: 0.25rem 0;
            color: #666;
        }
        .quick-action-btn {
            display: inline-block;
            padding: 0.75rem 1.5rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .quick-action-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.2);
        }
        .footer {
            background: #f8f9fa;
            padding: 2rem;
            text-align: center;
            color: #666;
            font-size: 0.9rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
        }
        th {
            background: #f8f9fa;
            font-weight: bold;
            color: #667eea;
        }
        .metric-good { color: #4caf50; font-weight: bold; }
        .metric-warning { color: #f39c12; font-weight: bold; }
        .metric-bad { color: #e74c3c; font-weight: bold; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><span class="icon">âš¡</span> Performance Analysis Report</h1>
            <p>Static analysis of tkr-docusearch codebase for performance bottlenecks</p>
            <p style="font-size: 0.9rem; margin-top: 0.5rem;">Generated: 2025-10-16 | Project Version: 0.9.0</p>
        </div>

        <div class="content">
            <!-- Executive Summary -->
            <div class="section">
                <h2>Executive Summary</h2>
                <div class="score-card">
                    <div class="score-item score-good">
                        <span class="value">78/100</span>
                        <span class="label">Overall Performance Score</span>
                    </div>
                    <div class="score-item">
                        <span class="value">24.6k</span>
                        <span class="label">Lines of Code</span>
                    </div>
                    <div class="score-item score-warning">
                        <span class="value">12</span>
                        <span class="label">Issues Found</span>
                    </div>
                    <div class="score-item">
                        <span class="value">8</span>
                        <span class="label">High Priority</span>
                    </div>
                </div>

                <div class="stat-grid">
                    <div class="stat-box">
                        <h4>Severity Breakdown</h4>
                        <ul>
                            <li><strong>Critical:</strong> 0 issues</li>
                            <li><strong>High:</strong> 8 issues (-80 points)</li>
                            <li><strong>Medium:</strong> 3 issues (-15 points)</li>
                            <li><strong>Low:</strong> 1 issue (-2 points)</li>
                        </ul>
                    </div>
                    <div class="stat-box">
                        <h4>Primary Concerns</h4>
                        <ul>
                            <li>Missing GPU memory cache clearing</li>
                            <li>Inefficient metadata serialization</li>
                            <li>Sequential embedding decompression</li>
                            <li>Excessive logging in hot paths</li>
                            <li>Missing torch.no_grad() decorators</li>
                        </ul>
                    </div>
                    <div class="stat-box">
                        <h4>Performance Metrics</h4>
                        <ul>
                            <li class="metric-good">Search: 239ms avg (target: 300ms) âœ“</li>
                            <li class="metric-good">Image embedding: 2.3s (target: 6s) âœ“</li>
                            <li class="metric-good">Text embedding: 0.24s (target: 6s) âœ“</li>
                            <li class="metric-warning">Memory optimization needed</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Quick Actions -->
            <div class="section">
                <h2>Quick Actions</h2>
                <div style="display: flex; gap: 1rem; flex-wrap: wrap; margin: 1.5rem 0;">
                    <a href="#" onclick="alert('Generate plan:\n\nImplement GPU memory management strategy with torch.cuda.empty_cache() and torch.mps.empty_cache() after batch processing'); return false;" class="quick-action-btn">
                        ðŸš€ Optimize GPU Memory
                    </a>
                    <a href="#" onclick="alert('Generate plan:\n\nAdd batch decompression and caching for embeddings in search pipeline'); return false;" class="quick-action-btn">
                        ðŸ’¾ Fix Decompression Bottleneck
                    </a>
                    <a href="#" onclick="alert('Generate plan:\n\nOptimize metadata serialization with lazy loading and selective field inclusion'); return false;" class="quick-action-btn">
                        ðŸ“Š Reduce Metadata Overhead
                    </a>
                </div>
            </div>

            <!-- Memory Management Issues -->
            <div class="section">
                <h2>1. Memory Management Issues</h2>
                <p>Critical memory management patterns that can lead to GPU memory exhaustion and performance degradation.</p>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Missing torch.no_grad() Context Managers</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/embeddings/model_loader.py:152</p>
                    <p><strong>Impact:</strong> Unnecessary gradient computation and memory allocation during inference operations</p>
                    <div class="code-block">
<span class="line-num">150</span>    # Move to device<br>
<span class="line-num">151</span>    processed = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v<br>
<span class="line-num">152</span>                for k, v in processed.items()}<br>
<span class="line-num">153</span><br>
<span class="line-num">154</span>    # Generate embeddings<br>
<span class="line-num">155</span>    with torch.no_grad():  # âœ“ Good: Only in one location<br>
<span class="line-num">156</span>        embeddings_tensor = self.model(**processed)
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Add @torch.no_grad() decorator to all inference methods (embed_images, embed_texts, embed_query) at the ColPaliEngine level to ensure no gradient tracking across the entire pipeline.
                    </div>
                </div>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Insufficient GPU Memory Cache Management</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/embeddings/colpali_wrapper.py:396-407</p>
                    <p><strong>Impact:</strong> GPU memory fragmentation during batch processing, potential OOM errors</p>
                    <div class="code-block">
<span class="line-num">392</span>    def clear_cache(self):<br>
<span class="line-num">393</span>        """Clear GPU memory cache (useful between large batches)."""<br>
<span class="line-num">394</span>        try:<br>
<span class="line-num">395</span>            import torch<br>
<span class="line-num">396</span>            if self.config.device == 'mps' and torch.backends.mps.is_available():<br>
<span class="line-num">397</span>                torch.mps.empty_cache()  # âš  Method exists but never called!<br>
<span class="line-num">398</span>            elif self.config.device == 'cuda' and torch.cuda.is_available():<br>
<span class="line-num">399</span>                torch.cuda.empty_cache()<br>
<span class="line-num">400</span>        except Exception as e:<br>
<span class="line-num">401</span>            logger.warning(f"Failed to clear cache: {e}")
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Call clear_cache() automatically after each batch in embed_images() and embed_texts() methods. Add parameter to control frequency (e.g., clear_every_n_batches=5).
                    </div>
                </div>

                <div class="issue medium">
                    <div class="issue-header">
                        <div class="issue-title">Large Embedding Arrays Kept in Memory</div>
                        <span class="severity medium">MEDIUM</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/processing/processor.py:241-245</p>
                    <p><strong>Impact:</strong> Memory accumulation during document processing, especially for multi-page PDFs</p>
                    <div class="code-block">
<span class="line-num">241</span>    visual_results = self.visual_processor.process_pages(<br>
<span class="line-num">242</span>        pages=parsed_doc.pages,  # All pages loaded at once<br>
<span class="line-num">243</span>        doc_id=doc_id,<br>
<span class="line-num">244</span>        progress_callback=visual_progress_callback<br>
<span class="line-num">245</span>    )
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Implement streaming processing where pages are processed and stored one batch at a time, releasing memory between batches. Use generator pattern for parsed_doc.pages.
                    </div>
                </div>
            </div>

            <!-- Algorithmic Inefficiencies -->
            <div class="section">
                <h2>2. Algorithmic Inefficiencies</h2>
                <p>Suboptimal algorithms and data structure choices that impact performance.</p>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Sequential Embedding Decompression in Hot Path</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/storage/chroma_client.py:654-665</p>
                    <p><strong>Impact:</strong> O(n) decompression operations in search path, blocking Stage 2 re-ranking</p>
                    <div class="code-block">
<span class="line-num">654</span>    # Decompress full embeddings from metadata<br>
<span class="line-num">655</span>    full_embeddings = None<br>
<span class="line-num">656</span>    if 'full_embeddings' in metadata and 'seq_length' in metadata:<br>
<span class="line-num">657</span>        shape_str = metadata.get('embedding_shape', '')<br>
<span class="line-num">658</span>        if shape_str:<br>
<span class="line-num">659</span>            seq_len, dim = eval(shape_str)  # âš  eval() in hot path!<br>
<span class="line-num">660</span>            shape = (seq_len, dim)<br>
<span class="line-num">661</span>            full_embeddings = decompress_embeddings(<br>
<span class="line-num">662</span>                metadata['full_embeddings'],<br>
<span class="line-num">663</span>                shape<br>
<span class="line-num">664</span>            )  # Expensive gzip decompression for each result
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Implement batch decompression using multiprocessing.Pool or concurrent.futures. Add LRU cache for recently decompressed embeddings. Replace eval() with json.loads() or ast.literal_eval() for safety.
                    </div>
                </div>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Inefficient MaxSim Scoring (No Batching)</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/embeddings/scoring.py:73-97</p>
                    <p><strong>Impact:</strong> Sequential matrix operations instead of batched GPU computation</p>
                    <div class="code-block">
<span class="line-num">93</span>    try:<br>
<span class="line-num">94</span>        scores = []<br>
<span class="line-num">95</span>        for doc_emb in document_embeddings:  # âš  Sequential loop<br>
<span class="line-num">96</span>            score = maxsim_score(query_embeddings, doc_emb)<br>
<span class="line-num">97</span>            scores.append(score)<br>
<span class="line-num">98</span>        return scores
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Implement batched MaxSim scoring using PyTorch tensors on GPU. Process all document embeddings in a single matrix operation: query (Q, 768) @ docs (N, D, 768) â†’ similarity (N, Q, D) â†’ max(dim=2) â†’ sum(dim=1) â†’ scores (N,).
                    </div>
                </div>

                <div class="issue medium">
                    <div class="issue-header">
                        <div class="issue-title">Nested Loops in Metadata Sanitization</div>
                        <span class="severity medium">MEDIUM</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/storage/compression.py:224-240</p>
                    <p><strong>Impact:</strong> O(n*m) complexity for large metadata dictionaries with nested structures</p>
                    <div class="code-block">
<span class="line-num">224</span>    sanitized = {}<br>
<span class="line-num">225</span>    for key, value in metadata.items():  # Outer loop<br>
<span class="line-num">226</span>        if value is None or isinstance(value, (str, int, float, bool)):<br>
<span class="line-num">227</span>            sanitized[key] = value<br>
<span class="line-num">228</span>        elif isinstance(value, list):<br>
<span class="line-num">229</span>            sanitized[key] = json.dumps(value, separators=(',', ':'))  # Serializes entire list<br>
<span class="line-num">230</span>        elif isinstance(value, dict):<br>
<span class="line-num">231</span>            sanitized[key] = json.dumps(value, separators=(',', ':'))  # Nested dict serialization
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Implement single-pass metadata flattening with pre-computed type checks. Cache frequently used metadata patterns. Consider protocol buffers or msgpack for faster serialization.
                    </div>
                </div>
            </div>

            <!-- I/O and Storage Performance -->
            <div class="section">
                <h2>3. I/O and Storage Performance</h2>
                <p>File I/O and database operations that could benefit from optimization.</p>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Synchronous Image Saving During Processing</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/processing/processor.py:604-611</p>
                    <p><strong>Impact:</strong> Blocks embedding generation while saving images to disk</p>
                    <div class="code-block">
<span class="line-num">604</span>    embedding_id = self.storage_client.add_visual_embedding(<br>
<span class="line-num">605</span>        doc_id=result.doc_id,<br>
<span class="line-num">606</span>        page=result.page_num,<br>
<span class="line-num">607</span>        full_embeddings=result.embedding,<br>
<span class="line-num">608</span>        metadata=metadata,<br>
<span class="line-num">609</span>        image_path=image_path,  # âš  Synchronous I/O<br>
<span class="line-num">610</span>        thumb_path=thumb_path   # âš  Synchronous I/O<br>
<span class="line-num">611</span>    )
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Use asyncio for concurrent I/O operations. Save images in background thread pool while processing continues. Implement write-ahead log for crash recovery.
                    </div>
                </div>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">Large Metadata JSON Serialization</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/storage/chroma_client.py:351-361</p>
                    <p><strong>Impact:</strong> Debug JSON serialization on every embedding insert, even when not logging</p>
                    <div class="code-block">
<span class="line-num">351</span>    import json<br>
<span class="line-num">352</span>    try:<br>
<span class="line-num">353</span>        metadata_json = json.dumps(complete_metadata)  # âš  Always runs!<br>
<span class="line-num">354</span>        metadata_size_kb = len(metadata_json) / 1024<br>
<span class="line-num">355</span>        if metadata_size_kb > 50:<br>
<span class="line-num">356</span>            logger.warning(...)  # Only warns if > 50KB<br>
<span class="line-num">357</span>    except Exception as e:<br>
<span class="line-num">358</span>        logger.error(f"Failed to serialize metadata: {e}")
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Guard debug serialization with log level check: if logger.isEnabledFor(logging.DEBUG). Move to separate diagnostic function called on-demand.
                    </div>
                </div>

                <div class="issue medium">
                    <div class="issue-header">
                        <div class="issue-title">Redundant Collection Stats Query</div>
                        <span class="severity medium">MEDIUM</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/storage/chroma_client.py:885-892</p>
                    <p><strong>Impact:</strong> Full collection scan to count unique documents</p>
                    <div class="code-block">
<span class="line-num">885</span>    visual_docs = self._visual_collection.get(include=["metadatas"])  # âš  Fetches ALL<br>
<span class="line-num">886</span>    unique_docs = set()<br>
<span class="line-num">887</span>    if visual_docs['metadatas']:<br>
<span class="line-num">888</span>        unique_docs = {<br>
<span class="line-num">889</span>            meta.get('doc_id')<br>
<span class="line-num">890</span>            for meta in visual_docs['metadatas']  # Set comprehension over all docs<br>
<span class="line-num">891</span>            if meta.get('doc_id')<br>
<span class="line-num">892</span>        }
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Maintain document count in separate metadata store (Redis/SQLite). Use ChromaDB's native count() API if available. Add caching with TTL.
                    </div>
                </div>
            </div>

            <!-- Logging Performance -->
            <div class="section">
                <h2>4. Logging Performance Impact</h2>
                <p>Excessive logging operations that degrade performance in hot paths.</p>

                <div class="issue high">
                    <div class="issue-header">
                        <div class="issue-title">String Formatting in Loop Before Log Level Check</div>
                        <span class="severity high">HIGH</span>
                    </div>
                    <p><strong>Location:</strong> Multiple locations in embedding and storage modules</p>
                    <p><strong>Impact:</strong> String formatting operations executed even when log level prevents output</p>
                    <div class="code-block">
<span class="line-num">129</span>    logger.debug(f"Processing batch {i//batch_size + 1}: {len(batch)} images")<br>
<span class="line-num">...</span><br>
<span class="line-num">206</span>    logger.debug(f"Processing batch {i//batch_size + 1}: {len(batch)} texts")<br>
<span class="line-num">...</span><br>
<span class="line-num">640</span>    logger.debug(f"Converted image: {img.size[0]}x{img.size[1]}, {len(text)} chars")
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Use lazy evaluation with logger.debug() by passing format args: logger.debug("Processing batch %d: %d images", batch_num, len(batch)). Or guard with if logger.isEnabledFor(logging.DEBUG).
                    </div>
                </div>

                <div class="issue low">
                    <div class="issue-header">
                        <div class="issue-title">Excessive Info Logging During Batch Processing</div>
                        <span class="severity low">LOW</span>
                    </div>
                    <p><strong>Location:</strong> /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/src/embeddings/colpali_wrapper.py:143, 220</p>
                    <p><strong>Impact:</strong> Minor overhead from logging every batch completion</p>
                    <div class="code-block">
<span class="line-num">143</span>    logger.info(f"Image embedding complete: {len(images)} images in {elapsed_ms:.1f}ms")<br>
<span class="line-num">220</span>    logger.info(f"Text embedding complete: {len(texts)} texts in {elapsed_ms:.1f}ms")
                    </div>
                    <div class="fix-recommendation">
                        <strong>Recommendation:</strong> Reduce to DEBUG level or implement rate-limiting for info logs (e.g., log every 10th batch).
                    </div>
                </div>
            </div>

            <!-- Performance Best Practices -->
            <div class="section">
                <h2>5. Positive Patterns & Strengths</h2>
                <p>Well-implemented performance optimizations that should be maintained.</p>

                <div style="background: #e8f5e9; padding: 1.5rem; border-radius: 8px; margin: 1rem 0;">
                    <h3 style="color: #2e7d32; margin-top: 0;">âœ“ Excellent Compression Strategy</h3>
                    <p>4x compression ratio achieved with gzip+base64 encoding for multi-vector embeddings, reducing storage from ~300KB to ~75KB per page.</p>
                    <div class="code-block">
<span class="line-num">52</span>    embeddings_bytes = embeddings.tobytes()<br>
<span class="line-num">53</span>    compressed_bytes = gzip.compress(embeddings_bytes, compresslevel=6)<br>
<span class="line-num">54</span>    encoded = base64.b64encode(compressed_bytes).decode('utf-8')
                    </div>
                </div>

                <div style="background: #e8f5e9; padding: 1.5rem; border-radius: 8px; margin: 1rem 0;">
                    <h3 style="color: #2e7d32; margin-top: 0;">âœ“ Efficient Two-Stage Search Architecture</h3>
                    <p>Stage 1 uses fast HNSW index with CLS tokens (50-100ms), Stage 2 applies late interaction only to top candidates (<100ms).</p>
                    <p><strong>Result:</strong> Average search latency of 239ms, 21% faster than 300ms target.</p>
                </div>

                <div style="background: #e8f5e9; padding: 1.5rem; border-radius: 8px; margin: 1rem 0;">
                    <h3 style="color: #2e7d32; margin-top: 0;">âœ“ Batched Embedding Generation</h3>
                    <p>Proper batching with configurable batch sizes (4 for visual, 8 for text) optimizes GPU utilization and achieves 2.6x faster than target performance.</p>
                </div>

                <div style="background: #e8f5e9; padding: 1.5rem; border-radius: 8px; margin: 1rem 0;">
                    <h3 style="color: #2e7d32; margin-top: 0;">âœ“ Metal Performance Shaders (MPS) Support</h3>
                    <p>Native M1/M2/M3 GPU acceleration providing 10-20x speedup over CPU processing.</p>
                </div>
            </div>

            <!-- Priority Fixes -->
            <div class="section">
                <h2>Top 5 Priority Fixes</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Priority</th>
                            <th>Issue</th>
                            <th>Location</th>
                            <th>Expected Impact</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><span class="severity high">HIGH</span></td>
                            <td>Implement batch decompression for embeddings</td>
                            <td>chroma_client.py:654-665</td>
                            <td>40-60% reduction in Stage 2 latency</td>
                        </tr>
                        <tr>
                            <td><span class="severity high">HIGH</span></td>
                            <td>Add GPU memory cache clearing after batches</td>
                            <td>colpali_wrapper.py:127-136</td>
                            <td>Prevent OOM, enable larger batch sizes</td>
                        </tr>
                        <tr>
                            <td><span class="severity high">HIGH</span></td>
                            <td>Implement batched MaxSim scoring on GPU</td>
                            <td>scoring.py:73-97</td>
                            <td>50-70% faster Stage 2 re-ranking</td>
                        </tr>
                        <tr>
                            <td><span class="severity high">HIGH</span></td>
                            <td>Guard debug JSON serialization with log level</td>
                            <td>chroma_client.py:351-361</td>
                            <td>20-30% faster embedding insertion</td>
                        </tr>
                        <tr>
                            <td><span class="severity high">HIGH</span></td>
                            <td>Add @torch.no_grad() decorators to inference</td>
                            <td>colpali_wrapper.py:85, 158, 235</td>
                            <td>15-20% memory reduction</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Performance Benchmarks -->
            <div class="section">
                <h2>Current Performance Benchmarks</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Operation</th>
                            <th>Current</th>
                            <th>Target</th>
                            <th>Status</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Image Embedding</td>
                            <td class="metric-good">2.3s/page</td>
                            <td>6s/page</td>
                            <td class="metric-good">âœ“ 2.6x faster</td>
                        </tr>
                        <tr>
                            <td>Text Embedding</td>
                            <td class="metric-good">0.24s/chunk</td>
                            <td>6s/chunk</td>
                            <td class="metric-good">âœ“ 25x faster</td>
                        </tr>
                        <tr>
                            <td>Search Latency (Avg)</td>
                            <td class="metric-good">239ms</td>
                            <td>300ms</td>
                            <td class="metric-good">âœ“ 21% faster</td>
                        </tr>
                        <tr>
                            <td>Stage 1 Retrieval</td>
                            <td class="metric-good">50-100ms</td>
                            <td>200ms</td>
                            <td class="metric-good">âœ“ 2x faster</td>
                        </tr>
                        <tr>
                            <td>Stage 2 Re-ranking</td>
                            <td class="metric-warning">~1ms/doc</td>
                            <td>0.5ms/doc</td>
                            <td class="metric-warning">âš  Can optimize</td>
                        </tr>
                        <tr>
                            <td>Compression Ratio</td>
                            <td class="metric-good">4x</td>
                            <td>3x</td>
                            <td class="metric-good">âœ“ 33% better</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Detailed Recommendations -->
            <div class="section">
                <h2>Implementation Recommendations</h2>

                <h3>1. GPU Memory Management Strategy</h3>
                <div class="code-block">
# src/embeddings/colpali_wrapper.py
def embed_images(self, images: List[Image.Image], batch_size: Optional[int] = None):
    for i in range(0, len(images), batch_size):
        batch = images[i:i + batch_size]
        embeddings, seq_lengths = self.model.embed_batch(batch, "visual")
        all_embeddings.extend(embeddings)

        # Clear cache every batch to prevent fragmentation
        if (i // batch_size) % 5 == 0:  # Every 5 batches
            self.clear_cache()

    return BatchEmbeddingOutput(...)
                </div>

                <h3>2. Batch Decompression with Caching</h3>
                <div class="code-block">
# src/storage/chroma_client.py
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

class ChromaClient:
    def __init__(self, ...):
        self._decompression_cache = lru_cache(maxsize=1000)(self._decompress_embedding)
        self._executor = ThreadPoolExecutor(max_workers=4)

    def _decompress_embedding(self, compressed_str, shape):
        return decompress_embeddings(compressed_str, shape)

    def search_visual(self, query_embedding, n_results, filters):
        # ... retrieve candidates ...

        # Batch decompress in parallel
        futures = []
        for candidate in candidates:
            future = self._executor.submit(
                self._decompress_embedding,
                candidate['metadata']['full_embeddings'],
                eval(candidate['metadata']['embedding_shape'])
            )
            futures.append(future)

        for candidate, future in zip(candidates, futures):
            candidate['full_embeddings'] = future.result()
                </div>

                <h3>3. Batched MaxSim on GPU</h3>
                <div class="code-block">
# src/embeddings/scoring.py
import torch

def batch_maxsim_scores_gpu(
    query_embeddings: np.ndarray,
    document_embeddings: List[np.ndarray],
    device: str = "mps"
) -> List[float]:
    """GPU-accelerated batched MaxSim scoring."""

    # Convert to tensors
    query_tensor = torch.from_numpy(query_embeddings).to(device)  # (Q, 768)

    # Pad documents to same length and stack
    max_len = max(d.shape[0] for d in document_embeddings)
    padded_docs = []
    for doc in document_embeddings:
        pad_len = max_len - doc.shape[0]
        padded = np.pad(doc, ((0, pad_len), (0, 0)), mode='constant')
        padded_docs.append(padded)

    docs_tensor = torch.from_numpy(np.stack(padded_docs)).to(device)  # (N, D, 768)

    # Batched similarity computation
    # query: (1, Q, 768), docs: (N, D, 768) -> (N, Q, D)
    similarity = torch.matmul(
        query_tensor.unsqueeze(0),  # (1, Q, 768)
        docs_tensor.transpose(1, 2)  # (N, 768, D)
    )  # (N, Q, D)

    # MaxSim: max over D, sum over Q
    max_sims = similarity.max(dim=2)[0]  # (N, Q)
    scores = max_sims.sum(dim=1)  # (N,)

    return scores.cpu().tolist()
                </div>

                <h3>4. Conditional Debug Logging</h3>
                <div class="code-block">
# src/storage/chroma_client.py
def add_visual_embedding(self, doc_id, page, full_embeddings, metadata, ...):
    # ... prepare metadata ...

    # Only serialize for debugging when needed
    if logger.isEnabledFor(logging.DEBUG):
        metadata_size_kb = len(json.dumps(complete_metadata)) / 1024
        if metadata_size_kb > 50:
            logger.debug(f"Large metadata: {metadata_size_kb:.1f}KB for {embedding_id}")

    # Store in ChromaDB
    self._visual_collection.add(...)
                </div>
            </div>

            <!-- Summary -->
            <div class="section">
                <h2>Summary & Next Steps</h2>
                <p>The tkr-docusearch codebase demonstrates strong architectural decisions with excellent performance metrics exceeding targets. However, several high-impact optimizations can further improve throughput and resource utilization:</p>

                <ul style="line-height: 2;">
                    <li><strong>Immediate Actions (Week 1):</strong> Implement GPU memory management, add batch decompression, guard debug serialization</li>
                    <li><strong>Short-term (Weeks 2-3):</strong> Migrate MaxSim to batched GPU computation, optimize metadata serialization</li>
                    <li><strong>Medium-term (Month 2):</strong> Implement streaming processing for large documents, add async I/O for image saving</li>
                    <li><strong>Long-term (Month 3+):</strong> Profile production workloads, implement adaptive batching based on available memory</li>
                </ul>

                <p style="margin-top: 1.5rem;"><strong>Expected Improvements:</strong> Implementing all high-priority fixes could reduce processing time by 40-60% and memory usage by 30-40%, enabling larger batch sizes and higher throughput.</p>
            </div>
        </div>

        <div class="footer">
            <p><strong>Performance Analysis Agent</strong> | tkr-context-kit v3.6.0</p>
            <p>Generated: 2025-10-16 | Analyzed: 24,618 lines of Python code</p>
            <p>Report Path: /Volumes/tkr-riffic/@tkr-projects/tkr-docusearch/.context-kit/analysis/reports/2025-10-16/performance-report.html</p>
        </div>
    </div>

    <script>
        // Quick action handlers
        function generatePlan(recommendation) {
            const message = `/create_plan ${recommendation} - ${window.location.pathname}`;
            if (confirm(`Generate implementation plan for: ${recommendation}?`)) {
                alert(`Copy this command to Claude Code:\n\n${message}`);
            }
        }

        // Add smooth scroll behavior
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });
    </script>
</body>
</html>
